{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GLiREL Notebook: Relationship Extraction on Label Studio Annotations\n",
    "\n",
    "This notebook demonstrates how to use the **GLiREL model** for relationship extraction (RE) on texts that have been annotated with entities in **Label Studio**.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Load Text & Annotations**: Read the original text and entity annotations from Label Studio JSON export\n",
    "2. **Prepare GLiREL Input**: Convert Label Studio annotations to GLiREL-compatible format\n",
    "3. **Relationship Extraction**: Use GLiREL to identify and classify relationships between entities\n",
    "4. **Analyze Results**: Display and export extracted relationships\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**Setup & Data Loading**\n",
    "- [Installation](#installation) - Install dependencies\n",
    "- [Load Example Data](#load-data) - Read text and Label Studio annotations\n",
    "- [Data Exploration](#explore-data) - Understand the structure\n",
    "\n",
    "**Data Preparation**\n",
    "- [Convert LS to GLiREL Format](#convert-format) - Prepare input for GLiREL model\n",
    "\n",
    "**Relationship Extraction**\n",
    "- [Extract Relations](#extract-relations) - Run GLiREL on prepared data\n",
    "\n",
    "---"
   ],
   "id": "c47aa752356bcff2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation {#installation}\n",
    "\n",
    "Install required packages for relationship extraction with GLiREL:\n"
   ],
   "id": "ff154d4d21ae89bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.441721Z",
     "start_time": "2026-02-04T15:12:41.423103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\"gliner\", \"pandas\", \"json\"]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"âœ“ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ“ {package} installed successfully\")\n",
    "        except:\n",
    "            print(f\"âš ï¸  Could not install {package} (may already be available)\")\n",
    "\n",
    "print(\"\\nâœ“ Installation complete\")\n"
   ],
   "id": "30bd80a345682645",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "âœ“ gliner already installed\n",
      "âœ“ pandas already installed\n",
      "âœ“ json already installed\n",
      "\n",
      "âœ“ Installation complete\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Example Data {#load-data}\n",
    "\n",
    "Load the example text and its Label Studio entity annotations:\n"
   ],
   "id": "9dd24bc2277cd0bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.472853Z",
     "start_time": "2026-02-04T15:12:41.459616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "TEXT_FILE = \"example_text.txt\"\n",
    "LS_ANNOTATIONS_FILE = \"example_text_LS_entities.json\"\n",
    "SCHEMA_FILE = \"../gliner_schema_template.json\"\n",
    "\n",
    "# Load the raw text\n",
    "print(\"Loading raw text file...\")\n",
    "with open(TEXT_FILE, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"âœ“ Text loaded: {len(raw_text)} characters\")\n",
    "print(f\"\\nðŸ“„ Text preview (first 300 chars):\")\n",
    "print(\"-\" * 60)\n",
    "print(raw_text[:300] + \"...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load Label Studio annotations\n",
    "print(\"\\n\\nLoading Label Studio annotations...\")\n",
    "with open(LS_ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "    ls_data = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Annotations loaded: {len(ls_data)} document(s)\")\n",
    "\n",
    "# Load schema for reference\n",
    "print(\"\\nLoading schema configuration...\")\n",
    "with open(SCHEMA_FILE, 'r', encoding='utf-8') as f:\n",
    "    schema_config = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Schema loaded: {schema_config['schema_name']}\")\n"
   ],
   "id": "b75a600f02313cad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw text file...\n",
      "âœ“ Text loaded: 3688 characters\n",
      "\n",
      "ðŸ“„ Text preview (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "Se sont rÃ©unis en assemblÃ©e gÃ©nÃ©rale extraordinaire tous les actionnaires de la sociÃ©tÃ© anonyme Ã©tablie Ã  Anvers sous la dÃ©nomination de Â« Caucasian ManganÃ¨se CÂ° Ltd Â», constituÃ©e par acte devant le notaire Leclef, soussignÃ©, en date du vingt et un octobre mil neuf cent et sept, publiÃ© aux annexes d...\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Loading Label Studio annotations...\n",
      "âœ“ Annotations loaded: 1 document(s)\n",
      "\n",
      "Loading schema configuration...\n",
      "âœ“ Schema loaded: Historical French NER\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration {#explore-data}\n",
    "\n",
    "Understand the structure of Label Studio annotations and extract entities:\n"
   ],
   "id": "1f1e18b2b9097e97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.488123Z",
     "start_time": "2026-02-04T15:12:41.476839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract annotations from Label Studio export\n",
    "# Label Studio format: list of task objects -> annotations -> results\n",
    "\n",
    "task = ls_data[0]  # Get first (and likely only) task\n",
    "annotations = task['annotations'][0]  # Get first annotation set\n",
    "results = annotations['result']  # Get entity annotations\n",
    "\n",
    "print(f\"Total entities in Label Studio export: {len(results)}\")\n",
    "print(f\"\\nEntity types found:\")\n",
    "\n",
    "# Count entities by type\n",
    "entity_types = {}\n",
    "for result in results:\n",
    "    labels = result['value']['labels']\n",
    "    if labels:\n",
    "        entity_type = labels[0]\n",
    "        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "\n",
    "for entity_type, count in sorted(entity_types.items()):\n",
    "    print(f\"  â€¢ {entity_type:20s}: {count:3d}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Sample entities (first 5):\")\n",
    "print(\"-\" * 80)\n",
    "for i, result in enumerate(results[:5]):\n",
    "    entity_text = result['value']['text']\n",
    "    entity_label = result['value']['labels'][0]\n",
    "    char_start = result['value']['start']\n",
    "    char_end = result['value']['end']\n",
    "    confidence = result['value'].get('score', 0)\n",
    "\n",
    "    print(f\"{i+1}. [{char_start:4d}-{char_end:4d}] {entity_text:30s} | {entity_label:20s} (conf: {confidence:.3f})\")\n"
   ],
   "id": "f1d3cd7929f905e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities in Label Studio export: 79\n",
      "\n",
      "Entity types found:\n",
      "  â€¢ ADDRESS             :   2\n",
      "  â€¢ ARCHIVAL_REFERENCE  :   4\n",
      "  â€¢ CAPITAL_TYPE        :   1\n",
      "  â€¢ CITY                :  13\n",
      "  â€¢ CORPORATE_TITLE     :   8\n",
      "  â€¢ DATE                :   6\n",
      "  â€¢ HONORIFICS          :  10\n",
      "  â€¢ LEGAL_PROCEDURE     :   6\n",
      "  â€¢ LEGAL_STRUCTURE     :   2\n",
      "  â€¢ MISSION_STATEMENT   :   1\n",
      "  â€¢ ORGANIZATION        :   1\n",
      "  â€¢ PERSON              :  18\n",
      "  â€¢ PROFESSION          :   4\n",
      "  â€¢ REGISTERED_OFFICE   :   1\n",
      "  â€¢ SHARE_QUANTITY      :   1\n",
      "  â€¢ SHARE_TYPE          :   1\n",
      "\n",
      "ðŸ“‹ Sample entities (first 5):\n",
      "--------------------------------------------------------------------------------\n",
      "1. [1767-1779] 4 avril 1909                   | DATE                 (conf: 0.000)\n",
      "2. [2399-2411] 22 juin 1909                   | DATE                 (conf: 0.000)\n",
      "3. [3576-3588] 15 juin 1909                   | DATE                 (conf: 0.000)\n",
      "4. [2802-2814] 24 juin 1909                   | DATE                 (conf: 0.000)\n",
      "5. [ 320- 354] six novembre mil neuf cent et sept | DATE                 (conf: 0.000)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convert Label Studio to GLiREL Format {#convert-format}\n",
    "\n",
    "Transform Label Studio entity annotations into GLiREL-compatible input format.\n",
    "\n",
    "### GLiREL Input Format\n",
    "\n",
    "GLiREL expects a specific format for relationship extraction. It needs:\n",
    "\n",
    "1. **Text**: The original document text\n",
    "2. **Entities**: A structured list of entities with their positions and types\n",
    "3. **Relations**: (Optional for pre-annotation) Known relations between entities\n",
    "\n",
    "The standard format is:\n",
    "```json\n",
    "{\n",
    "  \"text\": \"...\",\n",
    "  \"entities\": [\n",
    "    {\"id\": \"...\", \"type\": \"...\", \"start\": 0, \"end\": 10}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {\"head\": \"...\", \"tail\": \"...\", \"type\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ],
   "id": "b65fbd478e039c44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.503023Z",
     "start_time": "2026-02-04T15:12:41.493656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_entities_from_labelstudio(ls_data, text):\n",
    "    \"\"\"\n",
    "    Extract entity information from Label Studio JSON export and format for GLiREL.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ls_data : list\n",
    "        Label Studio exported JSON (list of tasks)\n",
    "    text : str\n",
    "        Original text\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : GLiREL-formatted data with entities\n",
    "    \"\"\"\n",
    "\n",
    "    task = ls_data[0]\n",
    "    annotations = task['annotations'][0]\n",
    "    results = annotations['result']\n",
    "\n",
    "    # Convert Label Studio results to GLiREL entity format\n",
    "    entities = []\n",
    "    entity_id_counter = 0\n",
    "\n",
    "    for result in results:\n",
    "        # Extract entity information from Label Studio format\n",
    "        entity_text = result['value']['text']\n",
    "        entity_type = result['value']['labels'][0]  # Get first label\n",
    "        char_start = result['value']['start']\n",
    "        char_end = result['value']['end']\n",
    "        confidence = result['value'].get('score', 0)\n",
    "\n",
    "        # Create GLiREL entity entry\n",
    "        gliren_entity = {\n",
    "            \"id\": f\"ent_{entity_id_counter}\",\n",
    "            \"type\": entity_type.upper(),\n",
    "            \"start\": char_start,\n",
    "            \"end\": char_end,\n",
    "            \"text\": entity_text,\n",
    "            \"confidence\": confidence,\n",
    "            \"ls_id\": result['id']  # Keep reference to original Label Studio ID\n",
    "        }\n",
    "\n",
    "        entities.append(gliren_entity)\n",
    "        entity_id_counter += 1\n",
    "\n",
    "    # Create GLiREL input format\n",
    "    gliren_input = {\n",
    "        \"text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"metadata\": {\n",
    "            \"source\": \"Label Studio\",\n",
    "            \"task_id\": task['id'],\n",
    "            \"annotation_id\": annotations['id'],\n",
    "            \"num_entities\": len(entities),\n",
    "            \"entity_types\": list(set(e['type'] for e in entities))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return gliren_input\n",
    "\n",
    "# Convert Label Studio annotations to GLiREL format\n",
    "print(\"Converting Label Studio annotations to GLiREL format...\")\n",
    "gliren_input = extract_entities_from_labelstudio(ls_data, raw_text)\n",
    "\n",
    "print(f\"âœ“ Conversion complete!\")\n",
    "print(f\"  Total entities: {gliren_input['metadata']['num_entities']}\")\n",
    "print(f\"  Entity types: {', '.join(gliren_input['metadata']['entity_types'])}\")\n"
   ],
   "id": "da0f5393e572c713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Label Studio annotations to GLiREL format...\n",
      "âœ“ Conversion complete!\n",
      "  Total entities: 79\n",
      "  Entity types: LEGAL_PROCEDURE, PERSON, LEGAL_STRUCTURE, DATE, SHARE_QUANTITY, CAPITAL_TYPE, MISSION_STATEMENT, REGISTERED_OFFICE, CITY, ADDRESS, ARCHIVAL_REFERENCE, PROFESSION, SHARE_TYPE, ORGANIZATION, CORPORATE_TITLE, HONORIFICS\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validate GLiREL Input Format\n",
    "\n",
    "Ensure the converted data is properly formatted and can be used by GLiREL:\n"
   ],
   "id": "59542936d0c65471"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.518222Z",
     "start_time": "2026-02-04T15:12:41.507023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Validate entity positions match the original text\n",
    "print(\"Validating entity positions...\")\n",
    "validation_passed = True\n",
    "errors = []\n",
    "\n",
    "for i, entity in enumerate(gliren_input['entities']):\n",
    "    entity_start = entity['start']\n",
    "    entity_end = entity['end']\n",
    "    entity_text_in_doc = raw_text[entity_start:entity_end]\n",
    "    entity_text_stored = entity['text']\n",
    "\n",
    "    # Check if entity position matches stored text\n",
    "    if entity_text_in_doc != entity_text_stored:\n",
    "        validation_passed = False\n",
    "        errors.append(\n",
    "            f\"Entity {i} ({entity['id']}): \"\n",
    "            f\"Text mismatch! \"\n",
    "            f\"In document: '{entity_text_in_doc}' \"\n",
    "            f\"vs Stored: '{entity_text_stored}'\"\n",
    "        )\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"âœ“ All entity positions are valid!\")\n",
    "    print(f\"  {len(gliren_input['entities'])} entities verified\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Found {len(errors)} validation errors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Display sample of converted entities\n",
    "print(f\"\\nðŸ“‹ Sample of converted GLiREL entities (first 5):\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for entity in gliren_input['entities'][:5]:\n",
    "    print(f\"  ID: {entity['id']:8s} | Type: {entity['type']:20s} | \"\n",
    "          f\"Pos: [{entity['start']:4d}-{entity['end']:4d}] | \"\n",
    "          f\"Text: '{entity['text']:30s}' | Conf: {entity['confidence']:.3f}\")\n"
   ],
   "id": "671a9d685fd26bcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating entity positions...\n",
      "âœ“ All entity positions are valid!\n",
      "  79 entities verified\n",
      "\n",
      "ðŸ“‹ Sample of converted GLiREL entities (first 5):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  ID: ent_0    | Type: DATE                 | Pos: [1767-1779] | Text: '4 avril 1909                  ' | Conf: 0.000\n",
      "  ID: ent_1    | Type: DATE                 | Pos: [2399-2411] | Text: '22 juin 1909                  ' | Conf: 0.000\n",
      "  ID: ent_2    | Type: DATE                 | Pos: [3576-3588] | Text: '15 juin 1909                  ' | Conf: 0.000\n",
      "  ID: ent_3    | Type: DATE                 | Pos: [2802-2814] | Text: '24 juin 1909                  ' | Conf: 0.000\n",
      "  ID: ent_4    | Type: DATE                 | Pos: [ 320- 354] | Text: 'six novembre mil neuf cent et sept' | Conf: 0.000\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save GLiREL Input Files\n",
    "\n",
    "Export the prepared data in formats suitable for GLiREL:\n"
   ],
   "id": "3e49f9d5fe3c2624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:12:41.533211Z",
     "start_time": "2026-02-04T15:12:41.522406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"gliren_input\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "gliren_json_file = output_dir / \"example_text_gliren_input.json\"\n",
    "metadata_file = output_dir / \"conversion_metadata.json\"\n",
    "\n",
    "print(\"Saving GLiREL input files...\")\n",
    "\n",
    "# 1. Save as JSON (single document)\n",
    "print(f\"\\n1. Saving as JSON: {gliren_json_file}\")\n",
    "with open(gliren_json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(gliren_input, f, indent=2, ensure_ascii=False)\n",
    "print(f\"   âœ“ {gliren_json_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# 2. Save as JSONL (one entry per line, for batch processing)\n",
    "print(f\"\\n2. Saving as JSONL: {gliren_jsonl_file}\")\n",
    "with open(gliren_jsonl_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(gliren_input, ensure_ascii=False) + '\\n')\n",
    "print(f\"   âœ“ {gliren_jsonl_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# 3. Save conversion metadata\n",
    "print(f\"\\n3. Saving metadata: {metadata_file}\")\n",
    "conversion_metadata = {\n",
    "    \"source_file\": TEXT_FILE,\n",
    "    \"ls_annotations_file\": LS_ANNOTATIONS_FILE,\n",
    "    \"schema_file\": SCHEMA_FILE,\n",
    "    \"conversion_timestamp\": datetime.now().isoformat(),\n",
    "    \"text_length\": len(raw_text),\n",
    "    \"num_entities\": len(gliren_input['entities']),\n",
    "    \"entity_types_found\": gliren_input['metadata']['entity_types'],\n",
    "    \"output_files\": {\n",
    "        \"json\": str(gliren_json_file),\n",
    "        \"jsonl\": str(gliren_jsonl_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(conversion_metadata, f, indent=2, ensure_ascii=False)\n",
    "print(f\"   âœ“ {metadata_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\nâœ“ All files saved to: {output_dir}/\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  â€¢ {gliren_json_file.name} - Full GLiREL input (JSON)\")\n",
    "print(f\"  â€¢ {gliren_jsonl_file.name} - GLiREL input for batch processing (JSONL)\")\n",
    "print(f\"  â€¢ {metadata_file.name} - Conversion metadata\")\n"
   ],
   "id": "5acf843909ec9dad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GLiREL input files...\n",
      "\n",
      "1. Saving as JSON: gliren_input\\example_text_gliren_input.json\n",
      "   âœ“ 19.0 KB\n",
      "\n",
      "2. Saving as JSONL: gliren_input\\example_text_gliren_input.jsonl\n",
      "   âœ“ 14.1 KB\n",
      "\n",
      "3. Saving metadata: gliren_input\\conversion_metadata.json\n",
      "   âœ“ 0.8 KB\n",
      "\n",
      "âœ“ All files saved to: gliren_input/\n",
      "\n",
      "Files created:\n",
      "  â€¢ example_text_gliren_input.json - Full GLiREL input (JSON)\n",
      "  â€¢ example_text_gliren_input.jsonl - GLiREL input for batch processing (JSONL)\n",
      "  â€¢ conversion_metadata.json - Conversion metadata\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n",
   "id": "18891fa9c3514f72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Relation Extraction with GLiREL\n",
    "\n",
    "Now that we have all the entities extracted and formatted, we can proceed to run the GLiREL model to identify relationships between these entities.\n",
    "\n",
    "### GLiREL labels\n",
    "To extract the relationships, GLiREL first has to know what types of relationships to look for. Therefore, you have to define the possible head and/or tail entity types and the possible relationship types in the schema file. The schema file should be formatted as follows:\n",
    "```json\n",
    "{\n",
    "  \"glirel_labels\": {\n",
    "    \"RELATION_NAME_1\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE_A\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE_B\"]\n",
    "    },\n",
    "    \"RELATION_NAME_2\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE_X\", \"ENTITY_TYPE_Y\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE_Z\"]\n",
    "    },\n",
    "    \"RELATION_NAME_3\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE\"]\n",
    "    },\n",
    "    \"no relation\": {}\n",
    "  }\n",
    "}\n",
    "```\n",
    "The 'gliren_imput' folder contains a relations schema file 'gliren_schema_relations.json'. These relations will be used in the following steps."
   ],
   "id": "d5a693ac5aa675e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:26:26.720475Z",
     "start_time": "2026-02-04T15:26:22.883163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install spacy\n",
    "!pip install glirel\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "a40b15ba9d5f9a02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "âœ“ spacy already installed\n",
      "âœ“ glirel already installed\n",
      "\n",
      "Downloading spacy model...\n",
      "âœ“ Spacy model downloaded successfully\n",
      "\n",
      "âœ“ Installation complete\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:30:08.136977Z",
     "start_time": "2026-02-04T15:30:07.965471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glirel import GLiREL\n",
    "import spacy\n",
    "\n",
    "model = GLiREL.from_pretrained(\"jackboyla/glirel_beta\", proxies=None, resume_download=False)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'Derren Nesbitt had a history of being cast in \"Doctor Who\", having played villainous warlord Tegana in the 1964 First Doctor serial \"Marco Polo\".'\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "labels = ['country of origin', 'licensed to broadcast to', 'father', 'followed by', 'characters']\n",
    "\n",
    "ner = [[26, 27, 'PERSON', 'Marco Polo'], [22, 23, 'Q2989412', 'First Doctor']] # 'type' is not used -- it can be any string!\n",
    "\n",
    "relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "print('Number of relations:', len(relations))\n",
    "\n",
    "sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "print(\"\\nDescending Order by Score:\")\n",
    "for item in sorted_data_desc:\n",
    "    print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n"
   ],
   "id": "328bdbfc80ecfd7d",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GLiREL._from_pretrained() missing 2 required keyword-only arguments: 'proxies' and 'resume_download'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mglirel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GLiREL\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mGLiREL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjackboyla/glirel_beta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDerren Nesbitt had a history of being cast in \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDoctor Who\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, having played villainous warlord Tegana in the 1964 First Doctor serial \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMarco Polo\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     85\u001B[0m         validate_repo_id(arg_value)\n\u001B[0;32m     87\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_legacy_arguments(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\huggingface_hub\\hub_mixin.py:559\u001B[0m, in \u001B[0;36mModelHubMixin.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, force_download, token, cache_dir, local_files_only, revision, **model_kwargs)\u001B[0m\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_hub_mixin_inject_config \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[0;32m    557\u001B[0m         model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m--> 559\u001B[0m instance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[0;32m    560\u001B[0m     model_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(model_id),\n\u001B[0;32m    561\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m    562\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    563\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[0;32m    564\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    565\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    567\u001B[0m )\n\u001B[0;32m    569\u001B[0m \u001B[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001B[39;00m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001B[39;00m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mgetattr\u001B[39m(instance, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_hub_mixin_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, {})):\n",
      "\u001B[1;31mTypeError\u001B[0m: GLiREL._from_pretrained() missing 2 required keyword-only arguments: 'proxies' and 'resume_download'"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9faf3beafcae28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
