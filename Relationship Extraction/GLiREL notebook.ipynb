{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GLiREL Notebook: Relationship Extraction on Label Studio Annotations\n",
    "\n",
    "This notebook demonstrates how to use the **GLiREL model** for relationship extraction (RE) on texts that have been annotated with entities in **Label Studio**.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Load Text & Annotations**: Read the original text and entity annotations from Label Studio JSON export\n",
    "2. **Prepare GLiREL Input**: Convert Label Studio annotations to GLiREL-compatible format\n",
    "3. **Relationship Extraction**: Use GLiREL to identify and classify relationships between entities\n",
    "4. **Analyze Results**: Display and export extracted relationships\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**Setup & Data Loading**\n",
    "- [Installation](#installation) - Install dependencies\n",
    "- [Load Example Data](#load-data) - Read text and Label Studio annotations\n",
    "- [Data Exploration](#explore-data) - Understand the structure\n",
    "\n",
    "**Data Preparation**\n",
    "- [Convert LS to GLiREL Format](#convert-format) - Prepare input for GLiREL model\n",
    "\n",
    "**Relationship Extraction**\n",
    "- [Extract Relations](#extract-relations) - Run GLiREL on prepared data\n",
    "\n",
    "---"
   ],
   "id": "c47aa752356bcff2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Installation {#installation}\n",
    "\n",
    "Install required packages for relationship extraction with GLiREL:\n"
   ],
   "id": "ff154d4d21ae89bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T08:45:46.391360Z",
     "start_time": "2026-02-06T08:45:34.855286Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install gliner",
   "id": "e30818a764b0ccfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gliner in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (0.2.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner) (4.67.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner) (0.2.1)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner) (1.23.2)\n",
      "Collecting transformers>=4.57.3\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "     ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.8/10.3 MB 25.3 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 2.2/10.3 MB 23.6 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.3/10.3 MB 26.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.4/10.3 MB 23.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.6/10.3 MB 25.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 6.9/10.3 MB 25.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.1/10.3 MB 24.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.3/10.3 MB 25.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.3/10.3 MB 25.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.3/10.3 MB 23.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface_hub>=0.21.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner) (1.4.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (6.0.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (0.21.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner) (2025.10.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner) (1.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm->gliner) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner) (2.2.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner) (0.7.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner) (2026.1.15)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner) (25.12.19)\n",
      "Requirement already satisfied: protobuf in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner) (6.33.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (2026.1.4)\n",
      "Requirement already satisfied: idna in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->gliner) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from coloredlogs->onnxruntime->gliner) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->gliner) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from typer-slim->huggingface_hub>=0.21.4->gliner) (8.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime->gliner) (3.5.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner) (1.3.1)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "Successfully installed transformers-5.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. preprocessing input data for GLiREL {#load-data}\n",
    "\n",
    "If you want to use your own data, you have to make sure that the data is in the correct format. The following code snippet shows how to two conversions:\n",
    "1. Convert GLiNER2 output to GLiREL input format\n",
    "2. Convert Label Studio annotations to GLiREL input format"
   ],
   "id": "2ebeb6d09075243e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (1) GLiNER2 output -> GLiREL input {#convert-gliner-to-glirel}\n",
    "While the GLiNER2 model gives an output that is similar to the GLiREL input format, some adjustments are needed to make it fully compatible. Specifically, we need to convert character-based entity spans to word-based spans.\n",
    "\n",
    "This code will convert the character spans from the GLiNER2 output to word spans suitable for GLiREL:"
   ],
   "id": "9dd24bc2277cd0bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_word_positions(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Get character positions of each whitespace-separated word.\n",
    "    Returns: List of tuples: [(word, start_char, end_char), ...]\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for match in re.finditer(r'\\S+', text):\n",
    "        words.append((match.group(), match.start(), match.end()))\n",
    "    return words\n",
    "\n",
    "\n",
    "def char_span_to_word_span(text: str, start_char: int, end_char: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Convert character span to word span indices (0-indexed, exclusive end).\n",
    "    \"\"\"\n",
    "    words = get_word_positions(text)\n",
    "\n",
    "    word_start = None\n",
    "    word_end = None\n",
    "\n",
    "    for i, (word, w_start, w_end) in enumerate(words):\n",
    "        # Find first word that overlaps with the character span\n",
    "        if word_start is None and w_end > start_char and w_start < end_char:\n",
    "            word_start = i + 1  # inclusive start\n",
    "        # Find last word that overlaps with the character span\n",
    "        if w_start < end_char and w_end > start_char:\n",
    "            word_end = i + 1  # exclusive end\n",
    "\n",
    "    return word_start, word_end\n",
    "\n",
    "\n",
    "def add_word_spans_to_labelstudio(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Add word_start and word_end to all entity annotations in Label Studio format.\n",
    "\n",
    "    Args:\n",
    "        data: List of Label Studio tasks\n",
    "\n",
    "    Returns:\n",
    "        Same structure with word_start and word_end added to each entity value\n",
    "    \"\"\"\n",
    "    for task in data:\n",
    "        text = task.get(\"data\", {}).get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Process annotations\n",
    "        for annotation in task.get(\"annotations\", []):\n",
    "            for result in annotation.get(\"result\", []):\n",
    "                if result.get(\"type\") == \"labels\":\n",
    "                    value = result.get(\"value\", {})\n",
    "                    start_char = value.get(\"start\")\n",
    "                    end_char = value.get(\"end\")\n",
    "\n",
    "                    if start_char is not None and end_char is not None:\n",
    "                        word_start, word_end = char_span_to_word_span(text, start_char, end_char)\n",
    "                        value[\"word_start\"] = word_start\n",
    "                        value[\"word_end\"] = word_end\n",
    "\n",
    "        # Process predictions\n",
    "        for prediction in task.get(\"predictions\", []):\n",
    "            for result in prediction.get(\"result\", []):\n",
    "                if result.get(\"type\") == \"labels\":\n",
    "                    value = result.get(\"value\", {})\n",
    "                    start_char = value.get(\"start\")\n",
    "                    end_char = value.get(\"end\")\n",
    "\n",
    "                    if start_char is not None and end_char is not None:\n",
    "                        word_start, word_end = char_span_to_word_span(text, start_char, end_char)\n",
    "                        value[\"word_start\"] = word_start\n",
    "                        value[\"word_end\"] = word_end\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# === MAIN: Load, convert, and save ===\n",
    "\n",
    "input_file = Path(\"../labelstudio_batch_20260203_150043.json\")\n",
    "output_file = Path(\"../Relationship Extraction/gliren_input\") / f\"{input_file.stem}_with_word_spans.json\"\n",
    "\n",
    "# Load Label Studio JSON\n",
    "print(f\"Loading: {input_file}\")\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} tasks\")\n",
    "\n",
    "# Add word spans\n",
    "data_with_word_spans = add_word_spans_to_labelstudio(data)\n",
    "\n",
    "# Count how many entities were processed\n",
    "total_entities = 0\n",
    "for task in data_with_word_spans:\n",
    "    for annotation in task.get(\"annotations\", []):\n",
    "        total_entities += sum(1 for r in annotation.get(\"result\", []) if r.get(\"type\") == \"labels\")\n",
    "    for prediction in task.get(\"predictions\", []):\n",
    "        total_entities += sum(1 for r in prediction.get(\"result\", []) if r.get(\"type\") == \"labels\")\n",
    "\n",
    "print(f\"Processed {total_entities} entity annotations\")\n",
    "\n",
    "# Save output\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_with_word_spans, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Saved to: {output_file}\")\n",
    "\n",
    "# Show sample of first entity with word spans\n",
    "if data_with_word_spans:\n",
    "    task = data_with_word_spans[0]\n",
    "    text = task.get(\"data\", {}).get(\"text\", \"\")\n",
    "    words = get_word_positions(text)\n",
    "\n",
    "    # Find first entity to show\n",
    "    sample = None\n",
    "    for annotation in task.get(\"annotations\", []):\n",
    "        for result in annotation.get(\"result\", []):\n",
    "            if result.get(\"type\") == \"labels\":\n",
    "                sample = result\n",
    "                break\n",
    "        if sample:\n",
    "            break\n",
    "    if not sample:\n",
    "        for prediction in task.get(\"predictions\", []):\n",
    "            for result in prediction.get(\"result\", []):\n",
    "                if result.get(\"type\") == \"labels\":\n",
    "                    sample = result\n",
    "                    break\n",
    "            if sample:\n",
    "                break\n",
    "\n",
    "    if sample:\n",
    "        v = sample[\"value\"]\n",
    "        print(f\"\\n=== Sample entity ===\")\n",
    "        print(f\"Text: '{v.get('text')}'\")\n",
    "        print(f\"Label: {v.get('labels')}\")\n",
    "        print(f\"Char span: [{v.get('start')}:{v.get('end')}]\")\n",
    "        print(f\"Word span: [{v.get('word_start')}:{v.get('word_end')}]\")\n",
    "\n",
    "        ws, we = v.get('word_start'), v.get('word_end')\n",
    "        if ws is not None and we is not None:\n",
    "            word_texts = [words[i][0] for i in range(ws, min(we, len(words)))]\n",
    "            print(f\"Words: {word_texts}\")"
   ],
   "id": "a2c890d8bbfcec5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### (2) Label Studio output -> GLiREL input {#convert-LS-to-GLiREL}\n",
    "\n",
    "Transform Label Studio entity annotations into GLiREL-compatible input format. For this, make sure that you export your Label Studio annotations in a **JSON-MIN** format.\n",
    "\n",
    "This code will read the Label Studio JSON export, convert character spans to word spans, and save the result in a GLiREL-compatible format:\n"
   ],
   "id": "b65fbd478e039c44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T16:10:59.844917Z",
     "start_time": "2026-02-05T16:10:59.815497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def get_word_positions(text: str) -> list:\n",
    "    \"\"\"Get character positions of each whitespace-separated word.\"\"\"\n",
    "    words = []\n",
    "    for match in re.finditer(r'\\S+', text):\n",
    "        words.append((match.group(), match.start(), match.end()))\n",
    "    return words\n",
    "\n",
    "\n",
    "def char_span_to_word_span(text: str, start_char: int, end_char: int) -> tuple:\n",
    "    \"\"\"Convert character span to word span indices (0-indexed, exclusive end).\"\"\"\n",
    "    words = get_word_positions(text)\n",
    "\n",
    "    word_start = None\n",
    "    word_end = None\n",
    "\n",
    "    for i, (word, w_start, w_end) in enumerate(words):\n",
    "        if word_start is None and w_end > start_char and w_start < end_char:\n",
    "            word_start = i+1  # inclusive start\n",
    "        if w_start < end_char and w_end > start_char:\n",
    "            word_end = i + 1\n",
    "\n",
    "    return word_start, word_end\n",
    "\n",
    "\n",
    "def add_word_spans_simple_format(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Add word_start and word_end to annotations in simple format:\n",
    "    [{\"text\": \"...\", \"id\": 123, \"label\": [{\"start\": 0, \"end\": 10, \"text\": \"...\", \"labels\": [...]}]}]\n",
    "    \"\"\"\n",
    "    for item in data:\n",
    "        text = item.get(\"text\", \"\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        for label_item in item.get(\"label\", []):\n",
    "            start_char = label_item.get(\"start\")\n",
    "            end_char = label_item.get(\"end\")\n",
    "\n",
    "            if start_char is not None and end_char is not None:\n",
    "                word_start, word_end = char_span_to_word_span(text, start_char, end_char)\n",
    "                label_item[\"word_start\"] = word_start\n",
    "                label_item[\"word_end\"] = word_end\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# === MAIN: Load, convert, and save ===\n",
    "\n",
    "input_file = Path(\"../Relationship Extraction/example_LS_output.json\")\n",
    "output_file = Path(\"../Relationship Extraction/gliren_input\") / f\"{input_file.stem}_LS_with_word_spans.json\"\n",
    "\n",
    "\n",
    "# Load JSON\n",
    "print(f\"Loading: {input_file}\")\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} documents\")\n",
    "\n",
    "# Add word spans\n",
    "data_with_word_spans = add_word_spans_simple_format(data)\n",
    "\n",
    "# Count entities\n",
    "total_entities = sum(len(item.get(\"label\", [])) for item in data_with_word_spans)\n",
    "print(f\"Processed {total_entities} entity annotations\")\n",
    "\n",
    "# Save output\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_with_word_spans, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Saved to: {output_file}\")\n",
    "\n",
    "# Show sample\n",
    "if data_with_word_spans and data_with_word_spans[0].get(\"label\"):\n",
    "    item = data_with_word_spans[0]\n",
    "    text = item[\"text\"]\n",
    "    words = get_word_positions(text)\n",
    "    label = item[\"label\"][0]\n",
    "\n",
    "    print(f\"\\n=== Sample entity ===\")\n",
    "    print(f\"Text: '{label.get('text')}'\")\n",
    "    print(f\"Label: {label.get('labels')}\")\n",
    "    print(f\"Char span: [{label.get('start')}:{label.get('end')}]\")\n",
    "    print(f\"Word span: [{label.get('word_start')}:{label.get('word_end')}]\")\n",
    "\n",
    "    ws, we = label.get('word_start'), label.get('word_end')\n",
    "    if ws is not None and we is not None:\n",
    "        word_texts = [words[i][0] for i in range(ws, min(we, len(words)))]\n",
    "        print(f\"Words: {word_texts}\")"
   ],
   "id": "a1849d4bdc7690ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ..\\Relationship Extraction\\example_LS_output.json\n",
      "Loaded 1 documents\n",
      "Processed 79 entity annotations\n",
      "✓ Saved to: ..\\Relationship Extraction\\gliren_input\\example_LS_output_LS_with_word_spans.json\n",
      "\n",
      "=== Sample entity ===\n",
      "Text: '4 avril 1909'\n",
      "Label: ['DATE']\n",
      "Char span: [1767:1779]\n",
      "Word span: [283:285]\n",
      "Words: ['avril', '1909.']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n",
   "id": "18891fa9c3514f72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Relation Extraction with GLiREL\n",
    "\n",
    "Now that we have all the entities extracted and formatted, we can proceed to run the GLiREL model to identify relationships between these entities.\n",
    "\n",
    "### GLiREL labels\n",
    "To extract the relationships, GLiREL first has to know what types of relationships to look for. Therefore, you have to define the possible head and/or tail entity types and the possible relationship types in the schema file. The schema file should be formatted as follows:\n",
    "```json\n",
    "{\n",
    "  \"glirel_labels\": {\n",
    "    \"RELATION_NAME_1\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE_A\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE_B\"]\n",
    "    },\n",
    "    \"RELATION_NAME_2\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE_X\", \"ENTITY_TYPE_Y\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE_Z\"]\n",
    "    },\n",
    "    \"RELATION_NAME_3\": {\n",
    "      \"allowed_head\": [\"ENTITY_TYPE\"],\n",
    "      \"allowed_tail\": [\"ENTITY_TYPE\"]\n",
    "    },\n",
    "    \"no relation\": {}\n",
    "  }\n",
    "}\n",
    "```\n",
    "The 'gliren_input' folder contains a relations schema file 'gliren_schema_relations.json'. These relations will be used in the following steps."
   ],
   "id": "d5a693ac5aa675e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### installing requirements\n",
    "To avoid a version mismatch, please install the following package versions:\n",
    "\n",
    "```json\n",
    "transformers==4.52.4\n",
    "huggingface-hub==0.36.1\n",
    "tokenizers==0.21.4\n",
    "```\n"
   ],
   "id": "98827c8144be65fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T08:46:25.342126Z",
     "start_time": "2026-02-06T08:46:05.543708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#installing specific package versions\n",
    "!pip install transformers==4.52.4\n",
    "!pip install huggingface-hub==0.36.1\n",
    "!pip install tokenizers==0.21.4\n",
    "\n",
    "!pip install spacy\n",
    "!pip install glirel\n",
    "!pip install loguru"
   ],
   "id": "a40b15ba9d5f9a02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.52.4\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Using cached huggingface_hub-0.36.1-py3-none-any.whl (566 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (3.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (2026.1.15)\n",
      "Requirement already satisfied: requests in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (2.2.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (4.67.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers==4.52.4) (0.7.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.52.4) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->transformers==4.52.4) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->transformers==4.52.4) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->transformers==4.52.4) (2.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->transformers==4.52.4) (3.11)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.4.0\n",
      "    Uninstalling huggingface_hub-1.4.0:\n",
      "      Successfully uninstalled huggingface_hub-1.4.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.2\n",
      "    Uninstalling tokenizers-0.22.2:\n",
      "      Successfully uninstalled tokenizers-0.22.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.1.0\n",
      "    Uninstalling transformers-5.1.0:\n",
      "      Successfully uninstalled transformers-5.1.0\n",
      "Successfully installed huggingface-hub-0.36.1 tokenizers-0.21.4 transformers-4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gliner 0.2.24 requires transformers>=4.57.3, but you have transformers 4.52.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub==0.36.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (0.36.1)\n",
      "Requirement already satisfied: requests in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (3.20.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (26.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (4.15.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub==0.36.1) (4.67.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.36.1) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub==0.36.1) (2.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub==0.36.1) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub==0.36.1) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub==0.36.1) (3.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.21.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tokenizers==0.21.4) (0.36.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.67.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (6.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.32.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.6.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (4.67.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (26.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glirel in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (0.36.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (4.52.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (4.5.0)\n",
      "Requirement already satisfied: torch in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (2.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (4.67.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from glirel) (1.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (2.32.5)\n",
      "Requirement already satisfied: fsspec[http]<=2025.10.0,>=2023.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (2025.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (3.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (6.0.3)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (0.70.18)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (26.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (3.6.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (0.28.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (23.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from datasets->glirel) (2.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub->glirel) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm->glirel) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from seqeval->glirel) (1.7.2)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch->glirel) (3.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch->glirel) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch->glirel) (3.1.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers->glirel) (0.7.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers->glirel) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers->glirel) (2026.1.15)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (3.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->glirel) (2026.1.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->glirel) (4.12.1)\n",
      "Requirement already satisfied: idna in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->glirel) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->glirel) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets->glirel) (0.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests>=2.32.2->datasets->glirel) (2.6.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from requests>=2.32.2->datasets->glirel) (3.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->glirel) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->glirel) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->glirel) (1.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->glirel) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from jinja2->torch->glirel) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pandas->datasets->glirel) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pandas->datasets->glirel) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from pandas->datasets->glirel) (2.9.0.post0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (25.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (6.7.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (1.8.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->glirel) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->glirel) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets->glirel) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from loguru) (1.2.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from loguru) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test with GLiREL #1",
   "id": "d370543fa1e7f99e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:45:32.980046Z",
     "start_time": "2026-02-05T10:45:17.319647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glirel import GLiREL\n",
    "import spacy\n",
    "\n",
    "model = GLiREL.from_pretrained(\"jackboyla/glirel-large-v0\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'Derren Nesbitt had a history of being cast in \"Doctor Who\", having played villainous warlord Tegana in the 1964 First Doctor serial \"Marco Polo\".'\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "labels = ['country of origin', 'licensed to broadcast to', 'father', 'followed by', 'characters']\n",
    "\n",
    "ner = [[26, 27, 'PERSON', 'Marco Polo'], [22, 23, 'Q2989412', 'First Doctor']] # 'type' is not used -- it can be any string!\n",
    "\n",
    "relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "print('Number of relations:', len(relations))\n",
    "\n",
    "sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(\"\\nDescending Order by Score:\")\n",
    "\n",
    "for item in sorted_data_desc:\n",
    "\n",
    "    print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n",
    "\n",
    "#expected output:\n",
    "\"\"\"\n",
    "Number of relations: 2\n",
    "\n",
    "Descending Order by Score:\n",
    "['First', 'Doctor'] --> followed by --> ['Marco', 'Polo'] | score: 0.0028011146932840347\n",
    "['Marco', 'Polo'] --> followed by --> ['First', 'Doctor'] | score: 0.0027413994539529085\n",
    "\"\"\"\n",
    "\n"
   ],
   "id": "328bdbfc80ecfd7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Heike\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Heike\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relations: 2\n",
      "\n",
      "Descending Order by Score:\n",
      "['First', 'Doctor'] --> followed by --> ['Marco', 'Polo'] | score: 0.0028011146932840347\n",
      "['Marco', 'Polo'] --> followed by --> ['First', 'Doctor'] | score: 0.0027413994539529085\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test with Spacy",
   "id": "40fb082da27c6667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:43:03.325993Z",
     "start_time": "2026-02-06T14:42:48.395300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from glirel import GLiREL\n",
    "\n",
    "# Load a blank spaCy model or an existing one\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add the GLiREL component to the pipeline\n",
    "nlp.add_pipe(\"glirel\", after=\"ner\")\n",
    "\n",
    "# Now you can use the pipeline with the GLiREL component\n",
    "text = \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976. The company is headquartered in Cupertino, California.\"\n",
    "\n",
    "labels = {\"glirel_labels\": {\n",
    "    'co-founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'country of origin': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'licensed to broadcast to': {\"allowed_head\": [\"ORG\"]},\n",
    "    'no relation': {},\n",
    "    'parent': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'followed by': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"PERSON\", \"ORG\"]},\n",
    "    'located in or next to body of water': {\"allowed_head\": [\"LOC\", \"GPE\", \"FAC\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'spouse': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'child': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'headquartered in': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\", \"FAC\"]},\n",
    "    'acquired by': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    'subsidiary of': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add the labels to the pipeline at inference time\n",
    "docs = list( nlp.pipe([(text, labels)], as_tuples=True) )\n",
    "relations = docs[0][0]._.relations\n",
    "\n",
    "print('Number of relations:', len(relations))\n",
    "\n",
    "sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "print(\"\\nDescending Order by Score:\")\n",
    "for item in sorted_data_desc:\n",
    "    print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n"
   ],
   "id": "ca6d9110d319681",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relations: 5\n",
      "\n",
      "Descending Order by Score:\n",
      "['Steve', 'Wozniak'] --> founder --> ['Apple', 'Inc.'] | score: 0.8068649768829346\n",
      "['Steve', 'Jobs'] --> founder --> ['Apple', 'Inc.'] | score: 0.8051494359970093\n",
      "['Ronald', 'Wayne'] --> founder --> ['Apple', 'Inc.'] | score: 0.7925519943237305\n",
      "['Apple', 'Inc.'] --> headquartered in --> ['California'] | score: 0.7537093758583069\n",
      "['Apple', 'Inc.'] --> headquartered in --> ['Cupertino'] | score: 0.7475748062133789\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test with GLiREL #3: no SpaCy used",
   "id": "e24c2eb4d216ea52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:48:34.504739Z",
     "start_time": "2026-02-06T14:48:16.405129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glirel import GLiREL\n",
    "import spacy\n",
    "\n",
    "model = GLiREL.from_pretrained(\"jackboyla/glirel-large-v0\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'Derren Nesbitt had a history of being cast in \"Doctor Who\", having played villainous warlord Tegana in the 1964 First Doctor serial \"Marco Polo\".'\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "labels = ['country of origin', 'licensed to broadcast to', 'father', 'followed by', 'characters']\n",
    "\n",
    "ner = [[26, 27, 'PERSON', 'Marco Polo'], [22, 23, 'Q2989412', 'First Doctor']] # 'type' is not used -- it can be any string!\n",
    "\n",
    "relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "print(relations)\n",
    "\n",
    "print('Number of relations:', len(relations))\n",
    "\n",
    "\n",
    "\n",
    "sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "print(\"\\nDescending Order by Score:\")\n",
    "for item in sorted_data_desc:\n",
    "    print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")"
   ],
   "id": "3c7ceac2659cff0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head_pos': [22, 24], 'tail_pos': [26, 28], 'head_text': ['First', 'Doctor'], 'tail_text': ['Marco', 'Polo'], 'label': 'followed by', 'score': 0.0028011146932840347}, {'head_pos': [26, 28], 'tail_pos': [22, 24], 'head_text': ['Marco', 'Polo'], 'tail_text': ['First', 'Doctor'], 'label': 'followed by', 'score': 0.0027413994539529085}]\n",
      "Number of relations: 2\n",
      "\n",
      "Descending Order by Score:\n",
      "['First', 'Doctor'] --> followed by --> ['Marco', 'Polo'] | score: 0.0028011146932840347\n",
      "['Marco', 'Polo'] --> followed by --> ['First', 'Doctor'] | score: 0.0027413994539529085\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:42:53.753935Z",
     "start_time": "2026-02-06T15:42:53.337608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glirel import GLiREL\n",
    "\n",
    "#model = GLiREL.from_pretrained(\"jackboyla/glirel-large-v0\")\n",
    "\n",
    "tokens = [\"L'assemblée\", \"désigne\", \"comme\", \"scrutateurs\", \"MM.\", \"Louis\", \"Cricquillon,\", \"administrateur\", \"délégué\", \"du\", \"Comptoir\", \"commercial\", \"anversois,\" , \"demeurant\", \"à\", \"Anvers,\", \"67,\", \"avenue\", \"des\", \"Arts,\", \"et\", \"Ferdinand\", \"De\", \"Bruyn,\", \"candidat-notaire,\", \"demeurant\", \"à\", \"Schooten.\"]\n",
    "\n",
    "# text = \"L'assemblée désigne comme scrutateurs MM. Louis Cricquillon, administrateur délégué du Comptoir commercial anversois, demeurant à Anvers, 67, avenue des Arts, et Ferdinand De Bruyn, candidat-notaire, demeurant à Schooten.\"\n",
    "\n",
    "\n",
    "labels = {\n",
    "    \"has occupation\": {\n",
    "      \"allowed_head\": [\"PERSON\"],\n",
    "      \"allowed_tail\": [\"OCCUPATION\"]\n",
    "    },\n",
    "    \"resides at\": {\n",
    "      \"allowed_head\": [\"PERSON\"],\n",
    "      \"allowed_tail\": [\"ADDRESS\", \"CITY\"]\n",
    "  },\n",
    "}\n",
    "\n",
    "# Keep original labels dict for constraint checking\n",
    "labels_and_constraints = labels\n",
    "# Extract just the relation names for the model\n",
    "relation_names = list(labels.keys())\n",
    "\n",
    "ner = [\n",
    "    [5, 6, \"PERSON\", \"Louis Cricquillon,\"],\n",
    "    [7, 8, \"OCCUPATION\", \"administrateur délégué\"],\n",
    "    [15, 15, \"CITY\", \"Anvers\"],\n",
    "    [20, 24, \"ADDRESS\", \"67 avenue des Arts\"],\n",
    "    [21, 23, \"PERSON\", \"Ferdinand De Bruyn,\"],\n",
    "    [24, 24, \"OCCUPATION\", \"candidat-notaire,\"],\n",
    "    [27, 27, \"CITY\", \"Schooten.\"]\n",
    "]\n",
    "\n",
    "from glirel.modules.utils import constrain_relations_by_entity_type\n",
    "from types import SimpleNamespace\n",
    "\n",
    "relations = model.predict_relations(tokens, relation_names, threshold=0.0, ner=ner, top_k=-1)\n",
    "\n",
    "# Create a mapping of (start, end) -> entity_label from the NER list\n",
    "# This maps token positions to their entity types for validation\n",
    "ner_lookup = {(start, end): entity_label for start, end, entity_label, _ in ner}\n",
    "print(ner_lookup)\n",
    "\n",
    "# Extract labels_and_constraints from the labels dict (which is still a dict, not a list)\n",
    "labels_and_constraints = labels\n",
    "\n",
    "# Filter relations to only include those that match allowed entity types\n",
    "filtered_relations = []\n",
    "for item in relations:\n",
    "    # Get the relation label\n",
    "    rel_label = item['label']\n",
    "\n",
    "    # Skip if this relation type is not in constraints\n",
    "    if rel_label not in labels_and_constraints:\n",
    "        continue\n",
    "\n",
    "    # Get the allowed head and tail entity types for this relation\n",
    "    allowed_head_types = labels_and_constraints[rel_label].get(\"allowed_head\", [])\n",
    "    allowed_tail_types = labels_and_constraints[rel_label].get(\"allowed_tail\", [])\n",
    "\n",
    "    # Extract head and tail positions from the relation\n",
    "    # head_pos and tail_pos are [start, end] in inclusive format\n",
    "    head_pos = tuple(item['head_pos'])\n",
    "    tail_pos = tuple(item['tail_pos'])\n",
    "\n",
    "    # Look up the entity types in the NER dictionary\n",
    "    head_entity_type = ner_lookup.get(head_pos)\n",
    "    tail_entity_type = ner_lookup.get(tail_pos)\n",
    "\n",
    "    # Only include this relation if both head and tail match allowed types\n",
    "    if head_entity_type and tail_entity_type:\n",
    "        if head_entity_type in allowed_head_types and tail_entity_type in allowed_tail_types:\n",
    "            filtered_relations.append(item)\n",
    "\n",
    "# Display filtered relations\n",
    "print(f\"Total relations found: {len(relations)}\")\n",
    "print(f\"Relations matching constraints: {len(filtered_relations)}\\n\")\n",
    "\n",
    "for item in filtered_relations:\n",
    "    print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "32307bfcf53d7da9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(5, 6): 'PERSON', (7, 8): 'OCCUPATION', (15, 15): 'CITY', (20, 24): 'ADDRESS', (21, 23): 'PERSON', (24, 24): 'OCCUPATION', (27, 27): 'CITY'}\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(5, 7)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(7, 9)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(15, 16)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(20, 25)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(21, 24)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(24, 25)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "(27, 28)\n",
      "Total relations found: 84\n",
      "Relations matching constraints: 0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:16:54.903209Z",
     "start_time": "2026-02-06T14:16:54.481541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Real-world example\n",
    "from glirel.modules.utils import constrain_relations_by_entity_type\n",
    "\n",
    "\n",
    "text = [\"Apple\", \"Inc\", \".\", \"was\", \"founded\", \"by\", \"Steve\", \"Jobs\", \",\", \"Steve\", \"Wozniak\", \",\", \"and\", \"Ronald\", \"Wayne\", \"in\", \"April\", \"1976\", \".\", \"The\", \"company\", \"is\", \"headquartered\", \"in\", \"Cupertino\",\",\", \"California\",\".\"]\n",
    "\n",
    "ner = [\n",
    "    [0, 1, \"ORG\", \"Apple Inc\"],\n",
    "    [6, 7, \"PERSON\", \"Steve Jobs\"],\n",
    "    [9, 10, \"PERSON\", \"Steve Wozniak\"],\n",
    "    [13, 14, \"PERSON\", \"Ronald Wayne\"],\n",
    "    [25, 25, \"GPE\", \"Cupertino\"],\n",
    "    [27, 27, \"GPE\", \"California\"]\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Convert NER list → spaCy‑free \"entity spans\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "entities = []\n",
    "\n",
    "for start, end, label, ent_text in ner:\n",
    "    entities.append({\n",
    "        \"start\": start,\n",
    "        \"end\": end + 1,     # end is inclusive in your NER, exclusive in spans\n",
    "        \"label\": label,\n",
    "        \"text\": ent_text,\n",
    "        \"tokens\": text[start:end+1]\n",
    "    })\n",
    "\n",
    "print(entities)\n",
    "\n",
    "# text = \"Jack Dorsey's father, Tim Dorsey, is a licensed pilot. Jack met his wife Sarah Paulson in New York in 2003. They have one son, Edward.\"\n",
    "\n",
    "labels = {\"glirel_labels\": {\n",
    "    'co-founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'country of origin': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'licensed to broadcast to': {\"allowed_head\": [\"ORG\"]},\n",
    "    'no relation': {},\n",
    "    'parent': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'followed by': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"PERSON\", \"ORG\"]},\n",
    "    'located in or next to body of water': {\"allowed_head\": [\"LOC\", \"GPE\", \"FAC\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'spouse': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'child': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'founded on date': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"DATE\"]},\n",
    "    'headquartered in': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\", \"FAC\"]},\n",
    "    'acquired by': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    'subsidiary of': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def predict_and_show(text, labels):\n",
    "    text = text\n",
    "    print(f\"Text: {text}\")\n",
    "\n",
    "    tokens = text\n",
    "\n",
    "    # NOTE: the end index should be inclusive\n",
    "    print(f\"Entities detected: {ner}\")\n",
    "\n",
    "    labels_and_constraints = None\n",
    "    if isinstance(labels, dict):\n",
    "        labels = labels[\"glirel_labels\"]\n",
    "        labels_and_constraints = labels\n",
    "        labels = list(labels.keys())\n",
    "\n",
    "    relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "    if isinstance(labels_and_constraints, dict):\n",
    "        print('Constraining relations by entity type')\n",
    "        # The constraint util expects spaCy-like ents (objects with .start, .end, .label_)\n",
    "        # Convert the raw ner list ([start, end_inclusive, label, text]) into simple objects\n",
    "        from types import SimpleNamespace\n",
    "        ents_for_constraints = [SimpleNamespace(start=s, end=e+1, label_=lab) for s, e, lab, _ in ner]\n",
    "\n",
    "        relations = constrain_relations_by_entity_type(ents_for_constraints, labels_and_constraints, relations)\n",
    "\n",
    "    print('Number of relations:', len(relations))\n",
    "\n",
    "    sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "    print(\"\\nDescending Order by Score:\")\n",
    "    for item in sorted_data_desc:\n",
    "        print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n",
    "\n",
    "predict_and_show(text, labels)"
   ],
   "id": "e22986f6643f6a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'end': 2, 'label': 'ORG', 'text': 'Apple Inc', 'tokens': ['Apple', 'Inc']}, {'start': 6, 'end': 8, 'label': 'PERSON', 'text': 'Steve Jobs', 'tokens': ['Steve', 'Jobs']}, {'start': 9, 'end': 11, 'label': 'PERSON', 'text': 'Steve Wozniak', 'tokens': ['Steve', 'Wozniak']}, {'start': 13, 'end': 15, 'label': 'PERSON', 'text': 'Ronald Wayne', 'tokens': ['Ronald', 'Wayne']}, {'start': 25, 'end': 26, 'label': 'GPE', 'text': 'Cupertino', 'tokens': [',']}, {'start': 27, 'end': 28, 'label': 'GPE', 'text': 'California', 'tokens': ['.']}]\n",
      "Text: ['Apple', 'Inc', '.', 'was', 'founded', 'by', 'Steve', 'Jobs', ',', 'Steve', 'Wozniak', ',', 'and', 'Ronald', 'Wayne', 'in', 'April', '1976', '.', 'The', 'company', 'is', 'headquartered', 'in', 'Cupertino', ',', 'California', '.']\n",
      "Entities detected: [[0, 1, 'ORG', 'Apple Inc'], [6, 7, 'PERSON', 'Steve Jobs'], [9, 10, 'PERSON', 'Steve Wozniak'], [13, 14, 'PERSON', 'Ronald Wayne'], [25, 25, 'GPE', 'Cupertino'], [27, 27, 'GPE', 'California']]\n",
      "Constraining relations by entity type\n",
      "Number of relations: 5\n",
      "\n",
      "Descending Order by Score:\n",
      "['Ronald', 'Wayne'] --> founder --> ['Apple', 'Inc'] | score: 0.807424008846283\n",
      "['Steve', 'Wozniak'] --> founder --> ['Apple', 'Inc'] | score: 0.8038501739501953\n",
      "['Steve', 'Jobs'] --> founder --> ['Apple', 'Inc'] | score: 0.7923585176467896\n",
      "['Apple', 'Inc'] --> headquartered in --> [','] | score: 0.6778672337532043\n",
      "['Apple', 'Inc'] --> headquartered in --> ['.'] | score: 0.6009151935577393\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:16:18.258667Z",
     "start_time": "2026-02-06T14:16:17.854359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Real-world example\n",
    "from glirel.modules.utils import constrain_relations_by_entity_type\n",
    "\n",
    "\n",
    "text = [\"L'assemblée\", \"désigne\", \"comme\", \"notaire\", \"MM\", \".\", \"Louis\", \"Cricquillon\"]\n",
    "\n",
    "ner = [\n",
    "    [6, 7, \"PERSON\", \"Louis Cricquillon\"],\n",
    "    [3, 3, \"OCCUPATION\", \"notaire\"],\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Convert NER list → spaCy‑free \"entity spans\"\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(entities)\n",
    "\n",
    "# text = \"Jack Dorsey's father, Tim Dorsey, is a licensed pilot. Jack met his wife Sarah Paulson in New York in 2003. They have one son, Edward.\"\n",
    "\n",
    "labels = {\"glirel_labels\": {\n",
    "    'co-founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'country of origin': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'licensed to broadcast to': {\"allowed_head\": [\"ORG\"]},\n",
    "    'no relation': {},\n",
    "    'parent': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'followed by': {\"allowed_head\": [\"PERSON\", \"ORG\"], \"allowed_tail\": [\"PERSON\", \"ORG\"]},\n",
    "    'has occupation': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"OCCUPATION\"]},\n",
    "    'located in or next to body of water': {\"allowed_head\": [\"LOC\", \"GPE\", \"FAC\"], \"allowed_tail\": [\"LOC\", \"GPE\"]},\n",
    "    'spouse': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'child': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"PERSON\"]},\n",
    "    'founder': {\"allowed_head\": [\"PERSON\"], \"allowed_tail\": [\"ORG\"]},\n",
    "    'founded on date': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"DATE\"]},\n",
    "    'headquartered in': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"LOC\", \"GPE\", \"FAC\"]},\n",
    "    'acquired by': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    'subsidiary of': {\"allowed_head\": [\"ORG\"], \"allowed_tail\": [\"ORG\", \"PERSON\"]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def predict_and_show(text, labels):\n",
    "    text = text\n",
    "    print(f\"Text: {text}\")\n",
    "\n",
    "    tokens = text\n",
    "\n",
    "    # NOTE: the end index should be inclusive\n",
    "    print(f\"Entities detected: {ner}\")\n",
    "\n",
    "    labels_and_constraints = None\n",
    "    if isinstance(labels, dict):\n",
    "        labels = labels[\"glirel_labels\"]\n",
    "        labels_and_constraints = labels\n",
    "        labels = list(labels.keys())\n",
    "\n",
    "    relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "    if isinstance(labels_and_constraints, dict):\n",
    "        print('Constraining relations by entity type')\n",
    "        # The constraint util expects spaCy-like ents (objects with .start, .end, .label_)\n",
    "        # Convert the raw ner list ([start, end_inclusive, label, text]) into simple objects\n",
    "        from types import SimpleNamespace\n",
    "        ents_for_constraints = [SimpleNamespace(start=s, end=e+1, label_=lab) for s, e, lab, _ in ner]\n",
    "\n",
    "        relations = constrain_relations_by_entity_type(ents_for_constraints, labels_and_constraints, relations)\n",
    "\n",
    "    print('Number of relations:', len(relations))\n",
    "\n",
    "    sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "    print(\"\\nDescending Order by Score:\")\n",
    "    for item in sorted_data_desc:\n",
    "        print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n",
    "\n",
    "predict_and_show(text, labels)"
   ],
   "id": "59907e651a258471",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'end': 2, 'label': 'ORG', 'text': 'Apple Inc', 'tokens': ['Apple', 'Inc']}, {'start': 6, 'end': 8, 'label': 'PERSON', 'text': 'Steve Jobs', 'tokens': ['Steve', 'Jobs']}, {'start': 9, 'end': 11, 'label': 'PERSON', 'text': 'Steve Wozniak', 'tokens': ['Steve', 'Wozniak']}, {'start': 13, 'end': 15, 'label': 'PERSON', 'text': 'Ronald Wayne', 'tokens': ['Ronald', 'Wayne']}, {'start': 25, 'end': 26, 'label': 'GPE', 'text': 'Cupertino', 'tokens': [',']}, {'start': 27, 'end': 28, 'label': 'GPE', 'text': 'California', 'tokens': ['.']}]\n",
      "Text: [\"L'assemblée\", 'désigne', 'comme', 'notaire', 'MM', '.', 'Louis', 'Cricquillon']\n",
      "Entities detected: [[6, 7, 'PERSON', 'Louis Cricquillon'], [3, 3, 'OCCUPATION', 'notaire']]\n",
      "Constraining relations by entity type\n",
      "Number of relations: 0\n",
      "\n",
      "Descending Order by Score:\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:22:20.104945Z",
     "start_time": "2026-02-06T14:22:19.776654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Real-world example\n",
    "from glirel.modules.utils import constrain_relations_by_entity_type\n",
    "\n",
    "text = [\"L'assemblée\", \"désigne\", \"comme\", \"scrutateurs\", \"MM\", \".\", \"Louis\", \"Cricquillon\", \",\", \"administrateur\", \"délégué\"]\n",
    "\n",
    "\n",
    "ner = [\n",
    "    [6, 7, \"PERSON\", \"Louis Cricquillon\"],\n",
    "    [9, 10, \"OCCUPATION\", \"administrateur délégué\"],\n",
    "]\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def ner_to_spacy_like(ner):\n",
    "    return [\n",
    "        SimpleNamespace(\n",
    "            start=start,\n",
    "            end=end + 1,      # spaCy-style exclusive\n",
    "            label_=label\n",
    "        )\n",
    "        for start, end, label, _ in ner\n",
    "    ]\n",
    "\n",
    "ents = ner_to_spacy_like(ner)\n",
    "\n",
    "# text = \"Jack Dorsey's father, Tim Dorsey, is a licensed pilot. Jack met his wife Sarah Paulson in New York in 2003. They have one son, Edward.\"\n",
    "\n",
    "labels = {\"glirel_labels\": {\n",
    "   \"HAS_OCCUPATION\": {\n",
    "      \"allowed_head\": [\"PERSON\"],\n",
    "      \"allowed_tail\": [\"OCCUPATION\"]\n",
    "    }\n",
    "}}\n",
    "\n",
    "\n",
    "def predict_and_show(text, labels):\n",
    "    print(f\"Text: {text}\")\n",
    "\n",
    "    tokens = text\n",
    "\n",
    "    # NOTE: the end index should be inclusive\n",
    "    print(f\"Entities detected: {ner}\")\n",
    "\n",
    "    labels_and_constraints = None\n",
    "    if isinstance(labels, dict):\n",
    "        labels = labels[\"glirel_labels\"]\n",
    "        labels_and_constraints = labels\n",
    "        labels = list(labels.keys())\n",
    "\n",
    "    relations = model.predict_relations(tokens, labels, threshold=0.0, ner=ner, top_k=1)\n",
    "\n",
    "    if isinstance(labels_and_constraints, dict):\n",
    "        print('Constraining relations by entity type')\n",
    "        # Create a mapping of (start, end) -> entity_label from the NER list\n",
    "        ner_lookup = {(start, end): entity_label for start, end, entity_label, _ in ner}\n",
    "\n",
    "        # Filter relations to only include those that match allowed entity types\n",
    "        filtered_relations = []\n",
    "        for item in relations:\n",
    "            rel_label = item['label']\n",
    "\n",
    "            # Skip if this relation type is not in constraints\n",
    "            if rel_label not in labels_and_constraints:\n",
    "                continue\n",
    "\n",
    "            # Get the allowed head and tail entity types for this relation\n",
    "            allowed_head_types = labels_and_constraints[rel_label].get(\"allowed_head\", [])\n",
    "            allowed_tail_types = labels_and_constraints[rel_label].get(\"allowed_tail\", [])\n",
    "\n",
    "            # Extract head and tail positions from the relation\n",
    "            head_pos = tuple(item['head_pos'])\n",
    "            tail_pos = tuple(item['tail_pos'])\n",
    "\n",
    "            # Look up the entity types in the NER dictionary\n",
    "            head_entity_type = ner_lookup.get(head_pos)\n",
    "            tail_entity_type = ner_lookup.get(tail_pos)\n",
    "\n",
    "            # Only include this relation if both head and tail match allowed types\n",
    "            if head_entity_type and tail_entity_type:\n",
    "                if head_entity_type in allowed_head_types and tail_entity_type in allowed_tail_types:\n",
    "                    filtered_relations.append(item)\n",
    "\n",
    "        relations = filtered_relations\n",
    "\n",
    "    print('Number of relations:', len(relations))\n",
    "\n",
    "    if relations:\n",
    "        sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "        print(\"\\nDescending Order by Score:\")\n",
    "        for item in sorted_data_desc:\n",
    "            print(f\"{item['head_text']} --> {item['label']} --> {item['tail_text']} | score: {item['score']}\")\n",
    "    else:\n",
    "        print(\"No relations found.\")\n",
    "\n",
    "predict_and_show(text, labels)\n"
   ],
   "id": "4887c18dfbd69c0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [\"L'assemblée\", 'désigne', 'comme', 'scrutateurs', 'MM', '.', 'Louis', 'Cricquillon', ',', 'administrateur', 'délégué']\n",
      "Entities detected: [[6, 7, 'PERSON', 'Louis Cricquillon'], [9, 10, 'OCCUPATION', 'administrateur délégué']]\n",
      "Constraining relations by entity type\n",
      "Number of relations: 1\n",
      "\n",
      "Descending Order by Score:\n",
      "['Louis', 'Cricquillon'] --> HAS_OCCUPATION --> ['administrateur', 'délégué'] | score: 0.06594593077898026\n"
     ]
    }
   ],
   "execution_count": 109
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
