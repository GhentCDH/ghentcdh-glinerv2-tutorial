{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Label Studio to GLiNER2 Training Data Conversion\n",
    "This code is used to convert the human labeled entities in Label Studio to a format that can be used for training GLiNER2. It reads the labeled data from Label Studio, extracts the relevant information, and creates a new JSON file that can be used for training.\n",
    "It follows the recommended format:\n",
    "```json\n",
    "{\"input\": \"text to process\", \"output\": {\"schema_definition\": \"with_annotations\"}}\n",
    "```"
   ],
   "id": "1d4cf9583109c1cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T09:44:51.407869Z",
     "start_time": "2026-02-27T09:44:51.384672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def convert_to_gliner2_format(annotation_data, schema):\n",
    "    \"\"\"\n",
    "    Converts Label Studio-style annotations to GLiNER2 format.\n",
    "    Only uses labels defined inside schema[\"entities\"].\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… Only take actual NER entity types\n",
    "    entity_types = list(schema[\"entities\"].keys())\n",
    "\n",
    "    gliner_data = []\n",
    "\n",
    "    for item in annotation_data:\n",
    "        text = item[\"text\"]\n",
    "        annotations = item.get(\"label\", [])\n",
    "\n",
    "        entities_dict = defaultdict(list)\n",
    "\n",
    "        # Collect entities\n",
    "        for ann in annotations:\n",
    "            entity_text = ann[\"text\"].strip()\n",
    "            labels = ann.get(\"labels\", [])\n",
    "\n",
    "            for label in labels:\n",
    "                # âœ… Only include labels that exist in schema\n",
    "                if label in entity_types:\n",
    "                    entities_dict[label].append(entity_text)\n",
    "\n",
    "        # Deduplicate while preserving order\n",
    "        for label in entities_dict:\n",
    "            seen = set()\n",
    "            deduped = []\n",
    "            for ent in entities_dict[label]:\n",
    "                if ent not in seen:\n",
    "                    deduped.append(ent)\n",
    "                    seen.add(ent)\n",
    "            entities_dict[label] = deduped\n",
    "\n",
    "        # Ensure ALL schema entity types are present (even if empty)\n",
    "        for entity_type in entity_types:\n",
    "            if entity_type not in entities_dict:\n",
    "                entities_dict[entity_type] = []\n",
    "\n",
    "        formatted_item = {\n",
    "            \"input\": text,\n",
    "            \"output\": {\n",
    "                \"entities\": dict(entities_dict)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        gliner_data.append(formatted_item)\n",
    "\n",
    "    return gliner_data\n",
    "\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    annotation_file = \"LS_labeled_data.json\"\n",
    "    schema_file = \"gliner_schema_hagiographics.json\"\n",
    "    output_file = \"gliner2_training_data.json\"\n",
    "\n",
    "    annotations = load_json(annotation_file)\n",
    "    schema = load_json(schema_file)\n",
    "\n",
    "    gliner_data = convert_to_gliner2_format(annotations, schema)\n",
    "    save_json(gliner_data, output_file)\n",
    "\n",
    "    print(f\"Converted {len(gliner_data)} examples.\")"
   ],
   "id": "53485300ff3cda20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 43 examples.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## finetuning GLiNER2 with LoRA\n",
    "Now that we have the training data in the correct format, we can use it to fine-tune GLiNER2 specifically for our use case."
   ],
   "id": "27aed93fba36b717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T15:06:03.599101Z",
     "start_time": "2026-02-27T15:06:01.126969Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install gliner2",
   "id": "c76f04ab6686966",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gliner2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: gliner in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner2) (0.2.24)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (2.10.0)\n",
      "Requirement already satisfied: transformers>=4.57.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (5.1.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.4 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (1.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (4.67.2)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (1.23.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from gliner->gliner2) (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from huggingface_hub>=0.21.4->gliner->gliner2) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner->gliner2) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner->gliner2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from torch>=2.0.0->gliner->gliner2) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->gliner->gliner2) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from tqdm->gliner->gliner2) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner->gliner2) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner->gliner2) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner->gliner2) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from transformers>=4.57.3->gliner->gliner2) (0.7.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.4->gliner->gliner2) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->gliner->gliner2) (3.0.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner->gliner2) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner->gliner2) (25.12.19)\n",
      "Requirement already satisfied: protobuf in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from onnxruntime->gliner->gliner2) (6.33.5)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from coloredlogs->onnxruntime->gliner->gliner2) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime->gliner->gliner2) (3.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\heike\\pycharmprojects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages (from typer-slim->huggingface_hub>=0.21.4->gliner->gliner2) (8.3.1)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T16:04:50.583770Z",
     "start_time": "2026-02-27T15:48:32.054608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gliner2 import GLiNER2\n",
    "from gliner2.training.trainer import GLiNER2Trainer, TrainingConfig\n",
    "import torch\n",
    "\n",
    "# Detect device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# LoRA configuration\n",
    "# -------------------------------\n",
    "\n",
    "config = TrainingConfig(\n",
    "    output_dir=\"./hagiography_lora\",\n",
    "    experiment_name=\"hagiography_latin_ner\",\n",
    "\n",
    "    # Training schedule\n",
    "    num_epochs=10,\n",
    "    batch_size=8 if device == \"cpu\" else 16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_ratio=0.1,\n",
    "    scheduler_type=\"cosine\",\n",
    "\n",
    "    # Learning rate (important)\n",
    "    task_lr=2e-4,\n",
    "\n",
    "    # LoRA settings\n",
    "    use_lora=True,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    lora_target_modules=[\"encoder\"],\n",
    "    save_adapter_only=True,\n",
    "\n",
    "    # Stability\n",
    "    fp16=(device == \"cuda\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_best=True,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=5,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load multilingual base model\n",
    "# -------------------------------\n",
    "\n",
    "model = GLiNER2.from_pretrained(\"fastino/gliner2-multi-v1\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create trainer\n",
    "# -------------------------------\n",
    "\n",
    "trainer = GLiNER2Trainer(model=model, config=config)\n",
    "\n",
    "# -------------------------------\n",
    "# Train from JSONL files\n",
    "# -------------------------------\n",
    "\n",
    "trainer.train(\n",
    "    train_data=\"train.jsonl\",\n",
    "    eval_data=\"val.jsonl\"\n",
    ")\n",
    "\n",
    "best_model = GLiNER2.from_pretrained(\"./hagiography_lora/best\")\n",
    "\n",
    "print(\"LoRA training complete.\")"
   ],
   "id": "c6e307d7e28c411d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/encoder_config/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/encoder_config%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/config.json \"HTTP/1.1 200 OK\"\n",
      "You are using a model of type extractor to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:32 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:33 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-27 16:48:33 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/fastino/gliner2-multi-v1/65627079a40a2b19a94eb4f08ebd00c429ccb6d4/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:33 - INFO - httpx - HTTP Request: GET https://huggingface.co/api/models/fastino/gliner2-multi-v1/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-27 16:48:33 - INFO - httpx - HTTP Request: GET https://huggingface.co/api/models/fastino/gliner2-multi-v1/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:34 - INFO - httpx - HTTP Request: GET https://huggingface.co/api/models/fastino/gliner2-multi-v1 \"HTTP/1.1 200 OK\"\n",
      "2026-02-27 16:48:41 - INFO - httpx - HTTP Request: HEAD https://huggingface.co/fastino/gliner2-multi-v1/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§  Model Configuration\n",
      "============================================================\n",
      "Encoder model      : microsoft/mdeberta-v3-base\n",
      "Counting layer     : count_lstm\n",
      "Token pooling      : first\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 16:48:41 - INFO - gliner2.training.trainer - Using device: cpu\n",
      "2026-02-27 16:48:41 - INFO - gliner2.training.trainer - Setting up LoRA for parameter-efficient fine-tuning...\n",
      "2026-02-27 16:48:41 - INFO - gliner2.training.trainer - Froze all model parameters for LoRA training\n",
      "2026-02-27 16:48:41 - INFO - gliner2.training.lora - Applied LoRA to 72 layers\n",
      "2026-02-27 16:48:42 - INFO - gliner2.training.trainer - LoRA setup complete: 1,327,104 trainable params out of 308,425,749 total (0.43%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”§ LoRA Configuration\n",
      "======================================================================\n",
      "Enabled            : True\n",
      "Rank (r)           : 8\n",
      "Alpha              : 16\n",
      "Scaling (Î±/r)      : 2.0000\n",
      "Dropout            : 0.05\n",
      "Target modules     : encoder\n",
      "LoRA layers        : 72\n",
      "----------------------------------------------------------------------\n",
      "Trainable params   : 1,327,104 / 308,425,749 (0.43%)\n",
      "Memory savings     : ~99.6% fewer gradients\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating records: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<?, ?record/s]\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer - Optimizer: LoRA params only = 144, LR=0.0002\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer - ***** Running Training *****\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Num examples = 34\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Num epochs = 10\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Batch size = 8\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Gradient accumulation steps = 2\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Effective batch size = 16\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Total optimization steps = 20\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   Warmup steps = 2\n",
      "2026-02-27 16:48:44 - INFO - gliner2.training.trainer -   LoRA enabled: 1,327,104 trainable / 308,425,749 total (0.43%)\n",
      "Training:  10%|â–ˆ         | 2/20 [03:08<25:14, 84.14s/it, loss=490.4298, lr=2.00e-04, samples/s=0.2, epoch=0.8] 2026-02-27 16:51:53 - INFO - gliner2.training.trainer - Epoch 1/10 - Loss: 216.5856\n",
      "2026-02-27 16:51:53 - INFO - gliner2.training.trainer - Running evaluation...\n",
      "\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.74s/it]\u001B[A\n",
      "Training:  10%|â–ˆ         | 2/20 [03:49<25:14, 84.14s/it, epoch=0.0]                                           2026-02-27 16:52:34 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\best\\adapter_weights.safetensors\n",
      "2026-02-27 16:52:34 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\best\\adapter_config.json\n",
      "2026-02-27 16:52:34 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\best\n",
      "2026-02-27 16:52:34 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'best' | step 2 | epoch 1.0 | 1,327,104 params | 5.1MB | 0.1s\n",
      "2026-02-27 16:52:34 - INFO - gliner2.training.trainer - New best eval_loss: 588.9455\n",
      "2026-02-27 16:52:36 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\checkpoint-epoch-1\\adapter_weights.safetensors\n",
      "2026-02-27 16:52:36 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\checkpoint-epoch-1\\adapter_config.json\n",
      "2026-02-27 16:52:36 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\checkpoint-epoch-1\n",
      "2026-02-27 16:52:36 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'checkpoint-epoch-1' | step 2 | epoch 1.0 | 1,327,104 params | 5.1MB | 0.0s\n",
      "2026-02-27 16:52:36 - INFO - gliner2.training.trainer - Removed old checkpoint: checkpoint-epoch-2\n",
      "Training:  20%|â–ˆâ–ˆ        | 4/20 [05:17<17:55, 67.22s/it, loss=380.6145, lr=1.94e-04, samples/s=0.2, epoch=1.8]2026-02-27 16:54:01 - INFO - gliner2.training.trainer - Epoch 2/10 - Loss: 207.7352\n",
      "2026-02-27 16:54:01 - INFO - gliner2.training.trainer - Running evaluation...\n",
      "\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.36s/it]\u001B[A\n",
      "Training:  20%|â–ˆâ–ˆ        | 4/20 [05:56<17:55, 67.22s/it, epoch=1.0]                                           2026-02-27 16:54:41 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\best\\adapter_weights.safetensors\n",
      "2026-02-27 16:54:41 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\best\\adapter_config.json\n",
      "2026-02-27 16:54:41 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\best\n",
      "2026-02-27 16:54:41 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'best' | step 4 | epoch 2.0 | 1,327,104 params | 5.1MB | 0.1s\n",
      "2026-02-27 16:54:41 - INFO - gliner2.training.trainer - New best eval_loss: 546.1636\n",
      "2026-02-27 16:54:42 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\checkpoint-epoch-2\\adapter_weights.safetensors\n",
      "2026-02-27 16:54:42 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\checkpoint-epoch-2\\adapter_config.json\n",
      "2026-02-27 16:54:42 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\checkpoint-epoch-2\n",
      "2026-02-27 16:54:42 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'checkpoint-epoch-2' | step 4 | epoch 2.0 | 1,327,104 params | 5.1MB | 0.0s\n",
      "2026-02-27 16:54:42 - INFO - gliner2.training.trainer - Removed old checkpoint: checkpoint-epoch-3\n",
      "Training:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [08:14<17:34, 75.35s/it, loss=416.4652, lr=1.77e-04, samples/s=0.2, epoch=2.8]2026-02-27 16:56:58 - INFO - gliner2.training.trainer - Epoch 3/10 - Loss: 202.3521\n",
      "2026-02-27 16:56:58 - INFO - gliner2.training.trainer - Running evaluation...\n",
      "\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.19s/it]\u001B[A\n",
      "Training:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [08:52<17:34, 75.35s/it, epoch=2.0]                                           2026-02-27 16:57:37 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\best\\adapter_weights.safetensors\n",
      "2026-02-27 16:57:37 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\best\\adapter_config.json\n",
      "2026-02-27 16:57:37 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\best\n",
      "2026-02-27 16:57:37 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'best' | step 6 | epoch 3.0 | 1,327,104 params | 5.1MB | 0.1s\n",
      "2026-02-27 16:57:37 - INFO - gliner2.training.trainer - New best eval_loss: 505.1870\n",
      "2026-02-27 16:57:38 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\checkpoint-epoch-3\\adapter_weights.safetensors\n",
      "2026-02-27 16:57:38 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\checkpoint-epoch-3\\adapter_config.json\n",
      "2026-02-27 16:57:38 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\checkpoint-epoch-3\n",
      "2026-02-27 16:57:38 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'checkpoint-epoch-3' | step 6 | epoch 3.0 | 1,327,104 params | 5.1MB | 0.0s\n",
      "2026-02-27 16:57:38 - INFO - gliner2.training.trainer - Removed old checkpoint: checkpoint-epoch-4\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [11:32<16:32, 82.67s/it, loss=446.6102, lr=1.50e-04, samples/s=0.2, epoch=3.8] 2026-02-27 17:00:17 - INFO - gliner2.training.trainer - Epoch 4/10 - Loss: 190.3527\n",
      "2026-02-27 17:00:17 - INFO - gliner2.training.trainer - Running evaluation...\n",
      "\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.80s/it]\u001B[A\n",
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [12:13<16:32, 82.67s/it, epoch=3.0]                                           2026-02-27 17:00:58 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\best\\adapter_weights.safetensors\n",
      "2026-02-27 17:00:58 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\best\\adapter_config.json\n",
      "2026-02-27 17:00:58 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\best\n",
      "2026-02-27 17:00:58 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'best' | step 8 | epoch 4.0 | 1,327,104 params | 5.1MB | 0.1s\n",
      "2026-02-27 17:00:58 - INFO - gliner2.training.trainer - New best eval_loss: 473.1496\n",
      "2026-02-27 17:00:59 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\checkpoint-epoch-4\\adapter_weights.safetensors\n",
      "2026-02-27 17:00:59 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\checkpoint-epoch-4\\adapter_config.json\n",
      "2026-02-27 17:00:59 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\checkpoint-epoch-4\n",
      "2026-02-27 17:00:59 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'checkpoint-epoch-4' | step 8 | epoch 4.0 | 1,327,104 params | 5.1MB | 0.0s\n",
      "2026-02-27 17:00:59 - INFO - gliner2.training.trainer - Removed old checkpoint: checkpoint-epoch-1\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [14:45<14:23, 86.38s/it, loss=339.9639, lr=1.17e-04, samples/s=0.2, epoch=4.8]2026-02-27 17:03:30 - INFO - gliner2.training.trainer - Epoch 5/10 - Loss: 189.4592\n",
      "2026-02-27 17:03:30 - INFO - gliner2.training.trainer - Running evaluation...\n",
      "\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.58s/it]\u001B[A\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [15:58<14:23, 86.38s/it, epoch=4.0]                                           2026-02-27 17:04:42 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\best\\adapter_weights.safetensors\n",
      "2026-02-27 17:04:42 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\best\\adapter_config.json\n",
      "2026-02-27 17:04:42 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\best\n",
      "2026-02-27 17:04:42 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'best' | step 10 | epoch 5.0 | 1,327,104 params | 5.1MB | 0.1s\n",
      "2026-02-27 17:04:42 - INFO - gliner2.training.trainer - New best eval_loss: 451.1935\n",
      "2026-02-27 17:04:44 - INFO - gliner2.training.trainer - Early stopping triggered at epoch 5\n",
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [15:59<15:59, 95.97s/it, epoch=4.0]\n",
      "2026-02-27 17:04:44 - INFO - gliner2.training.lora - Saved 144 LoRA tensors to hagiography_lora\\final\\adapter_weights.safetensors\n",
      "2026-02-27 17:04:44 - INFO - gliner2.training.lora - Saved adapter config to hagiography_lora\\final\\adapter_config.json\n",
      "2026-02-27 17:04:44 - INFO - gliner2.training.lora - Saved LoRA adapter to hagiography_lora\\final\n",
      "2026-02-27 17:04:44 - INFO - gliner2.training.trainer - ðŸ’¾ Saved adapter checkpoint 'final' | step 10 | epoch 5.0 | 1,327,104 params | 5.1MB | 0.0s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hagiography_lora/best\\config.json'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:419\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[1;32m--> 419\u001B[0m     \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    421\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    422\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    423\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    424\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    425\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:85\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m---> 85\u001B[0m         \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_legacy_arguments(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:129\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[1;34m(repo_id)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repo_id\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    132\u001B[0m     )\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
      "\u001B[1;31mHFValidationError\u001B[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hagiography_lora/best\\config.json'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 64\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# -------------------------------\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Train from JSONL files\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# -------------------------------\u001B[39;00m\n\u001B[0;32m     59\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m     60\u001B[0m     train_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     61\u001B[0m     eval_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 64\u001B[0m best_model \u001B[38;5;241m=\u001B[39m \u001B[43mGLiNER2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./hagiography_lora/best\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoRA training complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\gliner2\\model.py:529\u001B[0m, in \u001B[0;36mExtractor.from_pretrained\u001B[1;34m(cls, repo_or_dir, **kwargs)\u001B[0m\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hf_hub_download(repo, filename)\n\u001B[0;32m    528\u001B[0m config_path \u001B[38;5;241m=\u001B[39m download_or_local(repo_or_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 529\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m encoder_config_path \u001B[38;5;241m=\u001B[39m download_or_local(repo_or_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_config/config.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    532\u001B[0m encoder_config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(encoder_config_path)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:531\u001B[0m, in \u001B[0;36mPreTrainedConfig.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001B[0m\n\u001B[0;32m    528\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal_files_only\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m local_files_only\n\u001B[0;32m    529\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevision\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m revision\n\u001B[1;32m--> 531\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mget_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mbase_config_key \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mbase_config_key \u001B[38;5;129;01min\u001B[39;00m config_dict:\n\u001B[0;32m    533\u001B[0m     config_dict \u001B[38;5;241m=\u001B[39m config_dict[\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mbase_config_key]\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:569\u001B[0m, in \u001B[0;36mPreTrainedConfig.get_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    567\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[1;32m--> 569\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, kwargs\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:624\u001B[0m, in \u001B[0;36mPreTrainedConfig._get_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    620\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[1;32m--> 624\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    630\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    637\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, kwargs\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:276\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, **kwargs)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcached_file\u001B[39m(\n\u001B[0;32m    222\u001B[0m     path_or_repo_id: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m os\u001B[38;5;241m.\u001B[39mPathLike,\n\u001B[0;32m    223\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    224\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    225\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    226\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 276\u001B[0m     file \u001B[38;5;241m=\u001B[39m cached_files(path_or_repo_id\u001B[38;5;241m=\u001B[39mpath_or_repo_id, filenames\u001B[38;5;241m=\u001B[39m[filename], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    277\u001B[0m     file \u001B[38;5;241m=\u001B[39m file[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "File \u001B[1;32m~\\PycharmProjects\\ghentcdh-glinerv2-tutorial\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:468\u001B[0m, in \u001B[0;36mcached_files\u001B[1;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    463\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPermissionError at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m when downloading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    464\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    465\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2) a previous download was canceled and the lock file needs manual removal.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    466\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001B[39;00m\n\u001B[0;32m    471\u001B[0m resolved_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    472\u001B[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m full_filenames\n\u001B[0;32m    474\u001B[0m ]\n",
      "\u001B[1;31mOSError\u001B[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hagiography_lora/best\\config.json'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T16:05:25.301444Z",
     "start_time": "2026-02-27T16:05:23.216892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from gliner2 import GLiNER2\n",
    "\n",
    "extractor = best_model\n",
    "\n",
    "schema = create_gliner_schema_from_config_file(extractor, SCHEMA_CONFIG_PATH)\n",
    "\n",
    "text = \"\"\"\n",
    "[2] Impellitis itaque, ut sacratissimÃ¦ virginis Christi Glodesindis, cui devotis continue adhÃ¦retis excubiis, quÃ¦ conversationis initia, qui medii actus in finem usque feliciter consummatum exstiterint, quantum ex scriptis, quÃ¦ ad nostram Ã¦tatem qualibuscumque litteris annotata perdurant, queam advertere, stylo, quoquo possibile sit, audeam pertentare. Quod tanto tempore, tantaque instantia flagitatum, quia indecens videtur obniti, ope ejusdem prÃ¦stantissimÃ¦ Virginis, Deo spiritu mundo viventis virtutibus, tum vestris pariter fretus orationum subsidiis, etsi non sine quodam rubore, uti qui parcitatem proprii perpendam ingenii, non diu per longa moratus exordia, ocius narrationi accedam.\n",
    "\"\"\"\n",
    "\n",
    "results = extractor.extract(text, schema, threshold=0.1, include_confidence=True, include_spans=True,\n",
    "                            format_results=False)\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n"
   ],
   "id": "43d468d8363cf97d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"person\": [],\n",
      "      \"group\": [],\n",
      "      \"institution\": [\n",
      "        {\n",
      "          \"text\": \"mundo\",\n",
      "          \"confidence\": 0.589316725730896,\n",
      "          \"start\": 486,\n",
      "          \"end\": 491\n",
      "        }\n",
      "      ],\n",
      "      \"place\": [],\n",
      "      \"object\": [],\n",
      "      \"divine_entity\": [],\n",
      "      \"text_title\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
